# ğŸ—ï¸ Google Drive AI Organizer - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

## ğŸ“‹ ëª©ì°¨
- [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
- [ê¸°ìˆ  ìŠ¤íƒ](#ê¸°ìˆ -ìŠ¤íƒ)
- [ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°](#ì»´í¬ë„ŒíŠ¸-êµ¬ì¡°)
- [ë°ì´í„° í”Œë¡œìš°](#ë°ì´í„°-í”Œë¡œìš°)
- [ë³´ì•ˆ ì•„í‚¤í…ì²˜](#ë³´ì•ˆ-ì•„í‚¤í…ì²˜)
- [í™•ì¥ì„± ê³ ë ¤ì‚¬í•­](#í™•ì¥ì„±-ê³ ë ¤ì‚¬í•­)

## ì‹œìŠ¤í…œ ê°œìš”

Google Drive AI OrganizerëŠ” ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ëœ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.

### í•µì‹¬ ì„¤ê³„ ì›ì¹™

1. **ëª¨ë“ˆì„±**: ê° ì»´í¬ë„ŒíŠ¸ëŠ” ë…ë¦½ì ìœ¼ë¡œ ë°°í¬ ê°€ëŠ¥
2. **í™•ì¥ì„±**: ìˆ˜í‰ì  í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°
3. **ë³µì›ë ¥**: ì¥ì•  ê²©ë¦¬ ë° ìë™ ë³µêµ¬
4. **ë³´ì•ˆì„±**: ë‹¤ì¸µ ë³´ì•ˆ êµ¬ì¡°

## ê¸°ìˆ  ìŠ¤íƒ

### Backend
- **Framework**: FastAPI (Python 3.9+)
- **Authentication**: OAuth 2.0, JWT
- **Database**: SQLAlchemy + PostgreSQL
- **Cache**: Redis
- **Queue**: Celery + RabbitMQ
- **AI/ML**: Google Gemini AI

### Frontend
- **Framework**: React 18
- **State Management**: Redux Toolkit
- **UI Library**: Material-UI
- **Build Tool**: Vite
- **TypeScript**: v5.0+

### Infrastructure
- **Container**: Docker
- **Orchestration**: Kubernetes
- **CI/CD**: GitHub Actions
- **Monitoring**: Prometheus + Grafana
- **Logging**: ELK Stack

## ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°

```mermaid
graph TB
    subgraph "Client Layer"
        WEB[Web App]
        MOBILE[Mobile App]
        CLI[CLI Tool]
    end
    
    subgraph "API Gateway"
        GATEWAY[Kong/Nginx]
    end
    
    subgraph "Service Layer"
        AUTH[Auth Service]
        FILE[File Service]
        ORGANIZE[Organize Service]
        ANALYTICS[Analytics Service]
    end
    
    subgraph "Integration Layer"
        GDRIVE[Google Drive API]
        GEMINI[Gemini AI API]
    end
    
    subgraph "Data Layer"
        POSTGRES[(PostgreSQL)]
        REDIS[(Redis)]
        S3[Object Storage]
    end
    
    WEB --> GATEWAY
    MOBILE --> GATEWAY
    CLI --> GATEWAY
    
    GATEWAY --> AUTH
    GATEWAY --> FILE
    GATEWAY --> ORGANIZE
    GATEWAY --> ANALYTICS
    
    FILE --> GDRIVE
    ORGANIZE --> GEMINI
    
    AUTH --> POSTGRES
    AUTH --> REDIS
    FILE --> POSTGRES
    ORGANIZE --> POSTGRES
    ANALYTICS --> POSTGRES
```

## ë°ì´í„° í”Œë¡œìš°

### 1. ì¸ì¦ í”Œë¡œìš°

```python
@router.get("/auth/google")
async def google_auth():
    """
    1. í´ë¼ì´ì–¸íŠ¸ â†’ OAuth ìš”ì²­
    2. Google OAuth â†’ ì¸ì¦ ì½”ë“œ ë°˜í™˜
    3. ë°±ì—”ë“œ â†’ ì•¡ì„¸ìŠ¤ í† í° êµí™˜
    4. JWT í† í° ìƒì„± â†’ í´ë¼ì´ì–¸íŠ¸
    """
    flow = Flow.from_client_config(
        client_config=GOOGLE_OAUTH_CONFIG,
        scopes=GOOGLE_SCOPES
    )
    auth_url = flow.authorization_url()
    return RedirectResponse(auth_url)
```

### 2. íŒŒì¼ ë¶„ë¥˜ í”Œë¡œìš°

```python
async def classify_files(files: List[FileInfo]):
    """
    1. íŒŒì¼ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
    2. Gemini AI ë¶„ì„ ìš”ì²­
    3. ë¶„ë¥˜ ê²°ê³¼ ì²˜ë¦¬
    4. í´ë” êµ¬ì¡° ìƒì„±/ì—…ë°ì´íŠ¸
    5. íŒŒì¼ ì´ë™ ì‹¤í–‰
    """
    tasks = []
    for file in files:
        task = classify_single_file(file)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return process_classification_results(results)
```

### 3. ì‹¤ì‹œê°„ ë™ê¸°í™” í”Œë¡œìš°

```python
class DriveChangeDetector:
    """
    Google Drive ë³€ê²½ì‚¬í•­ ì‹¤ì‹œê°„ ê°ì§€
    """
    async def start_monitoring(self):
        page_token = await self.get_start_page_token()
        
        while True:
            changes = await self.fetch_changes(page_token)
            
            for change in changes:
                await self.process_change(change)
            
            page_token = changes.next_page_token
            await asyncio.sleep(10)  # 10ì´ˆ ê°„ê²© í´ë§
```

## ë³´ì•ˆ ì•„í‚¤í…ì²˜

### 1. ì¸ì¦ ë° ê¶Œí•œ ë¶€ì—¬

```python
class SecurityMiddleware:
    """
    ëª¨ë“  ìš”ì²­ì— ëŒ€í•œ ë³´ì•ˆ ê²€ì¦
    """
    def __init__(self):
        self.jwt_validator = JWTValidator()
        self.rate_limiter = RateLimiter()
        self.ip_filter = IPFilter()
    
    async def validate_request(self, request):
        # JWT ê²€ì¦
        if not self.jwt_validator.verify(request.headers):
            raise UnauthorizedException()
        
        # Rate Limiting
        if not self.rate_limiter.check(request.client):
            raise RateLimitException()
        
        # IP í•„í„°ë§
        if not self.ip_filter.is_allowed(request.client.host):
            raise ForbiddenException()
```

### 2. ë°ì´í„° ì•”í˜¸í™”

- **ì „ì†¡ ì¤‘**: TLS 1.3
- **ì €ì¥ ì‹œ**: AES-256-GCM
- **í‚¤ ê´€ë¦¬**: AWS KMS / Google Cloud KMS

### 3. ê°ì‚¬ ë¡œê¹…

```python
@contextmanager
def audit_log(action: str, user_id: str):
    """
    ëª¨ë“  ì¤‘ìš” ì‘ì—…ì— ëŒ€í•œ ê°ì‚¬ ë¡œê·¸
    """
    start_time = datetime.utcnow()
    
    try:
        yield
        log_entry = {
            "action": action,
            "user_id": user_id,
            "timestamp": start_time,
            "status": "success",
            "duration": (datetime.utcnow() - start_time).total_seconds()
        }
    except Exception as e:
        log_entry = {
            "action": action,
            "user_id": user_id,
            "timestamp": start_time,
            "status": "failed",
            "error": str(e)
        }
        raise
    finally:
        audit_logger.log(log_entry)
```

## í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

### 1. ìˆ˜í‰ì  í™•ì¥

```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gdrive-organizer
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    spec:
      containers:
      - name: api
        image: gdrive-organizer:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

### 2. ìºì‹± ì „ëµ

```python
class CacheStrategy:
    """
    ë‹¤ì¸µ ìºì‹± ì „ëµ
    """
    def __init__(self):
        self.l1_cache = InMemoryCache(ttl=60)  # 1ë¶„
        self.l2_cache = RedisCache(ttl=3600)    # 1ì‹œê°„
        self.l3_cache = CDNCache(ttl=86400)     # 1ì¼
    
    async def get(self, key: str):
        # L1 ìºì‹œ í™•ì¸
        if value := self.l1_cache.get(key):
            return value
        
        # L2 ìºì‹œ í™•ì¸
        if value := await self.l2_cache.get(key):
            self.l1_cache.set(key, value)
            return value
        
        # L3 ìºì‹œ í™•ì¸
        if value := await self.l3_cache.get(key):
            await self.l2_cache.set(key, value)
            self.l1_cache.set(key, value)
            return value
        
        return None
```

### 3. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

```sql
-- ì¸ë±ìŠ¤ ìµœì í™”
CREATE INDEX idx_files_user_modified 
ON files(user_id, modified_time DESC);

CREATE INDEX idx_files_folder_name 
ON files(folder_id, name);

-- íŒŒí‹°ì…”ë‹
CREATE TABLE files_2025 PARTITION OF files 
FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');

-- ì½ê¸° ì „ìš© ë³µì œë³¸
CREATE PUBLICATION files_pub FOR TABLE files;
CREATE SUBSCRIPTION files_sub 
CONNECTION 'host=replica1 dbname=gdrive' 
PUBLICATION files_pub;
```

### 4. ë¹„ë™ê¸° ì²˜ë¦¬

```python
class AsyncJobProcessor:
    """
    ëŒ€ìš©ëŸ‰ ì‘ì—… ë¹„ë™ê¸° ì²˜ë¦¬
    """
    def __init__(self):
        self.queue = asyncio.Queue()
        self.workers = []
    
    async def start(self, num_workers=5):
        for i in range(num_workers):
            worker = asyncio.create_task(self.worker(f"worker-{i}"))
            self.workers.append(worker)
    
    async def worker(self, name: str):
        while True:
            job = await self.queue.get()
            try:
                await self.process_job(job)
            except Exception as e:
                logger.error(f"{name} failed: {e}")
            finally:
                self.queue.task_done()
    
    async def add_job(self, job):
        await self.queue.put(job)
```

## ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

### 1. ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
from prometheus_client import Counter, Histogram, Gauge

# ë©”íŠ¸ë¦­ ì •ì˜
request_count = Counter('http_requests_total', 'Total HTTP requests')
request_duration = Histogram('http_request_duration_seconds', 'HTTP request duration')
active_users = Gauge('active_users', 'Number of active users')

@app.middleware("http")
async def metrics_middleware(request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    request_count.inc()
    request_duration.observe(time.time() - start_time)
    
    return response
```

### 2. ë¶„ì‚° ì¶”ì 

```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger import JaegerExporter

tracer = trace.get_tracer(__name__)

@tracer.start_as_current_span("organize_files")
async def organize_files(user_id: str):
    span = trace.get_current_span()
    span.set_attribute("user.id", user_id)
    
    with tracer.start_as_current_span("fetch_files"):
        files = await fetch_user_files(user_id)
    
    with tracer.start_as_current_span("classify_files"):
        classifications = await classify_files(files)
    
    with tracer.start_as_current_span("move_files"):
        results = await move_files(classifications)
    
    return results
```

### 3. ì—ëŸ¬ ì¶”ì 

```python
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration

sentry_sdk.init(
    dsn="YOUR_SENTRY_DSN",
    integrations=[FastApiIntegration()],
    traces_sample_rate=0.1,
    profiles_sample_rate=0.1,
)

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    sentry_sdk.capture_exception(exc)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error"}
    )
```

## ì¬í•´ ë³µêµ¬

### 1. ë°±ì—… ì „ëµ

```bash
#!/bin/bash
# backup.sh

# ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
pg_dump -h localhost -U gdrive_user gdrive_db > backup_$(date +%Y%m%d).sql

# S3 ì—…ë¡œë“œ
aws s3 cp backup_$(date +%Y%m%d).sql s3://gdrive-backups/

# 7ì¼ ì´ìƒ ëœ ë°±ì—… ì‚­ì œ
find . -name "backup_*.sql" -mtime +7 -delete
```

### 2. ë³µêµ¬ ì ˆì°¨

```python
class DisasterRecovery:
    """
    ì¬í•´ ë³µêµ¬ ìë™í™”
    """
    async def initiate_recovery(self):
        # 1. í—¬ìŠ¤ ì²´í¬
        if not await self.health_check():
            # 2. ë°±ì—…ì—ì„œ ë³µêµ¬
            await self.restore_from_backup()
            
            # 3. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
            await self.verify_data_integrity()
            
            # 4. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
            await self.restart_services()
            
            # 5. ì•Œë¦¼ ë°œì†¡
            await self.notify_team()
```

## ì„±ëŠ¥ ìµœì í™”

### 1. ì¿¼ë¦¬ ìµœì í™”

```python
# Bad
files = db.query(File).all()
for file in files:
    folder = db.query(Folder).filter_by(id=file.folder_id).first()

# Good
files = db.query(File).join(Folder).all()
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```python
async def batch_process_files(files: List[File], batch_size=100):
    """
    ëŒ€ëŸ‰ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬
    """
    for i in range(0, len(files), batch_size):
        batch = files[i:i+batch_size]
        tasks = [process_file(file) for file in batch]
        await asyncio.gather(*tasks)
        
        # Rate limiting
        await asyncio.sleep(1)
```

### 3. ì—°ê²° í’€ë§

```python
# database.py
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=40,
    pool_pre_ping=True,
    pool_recycle=3600
)
```

---

<p align="center">
  <i>ì´ ë¬¸ì„œëŠ” ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.</i>
</p>